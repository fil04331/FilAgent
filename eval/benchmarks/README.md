# FilAgent Benchmarks

This directory contains all benchmark harnesses for evaluating FilAgent performance.

## Structure

```
benchmarks/
â”œâ”€â”€ swe_bench/              # SWE-bench for software engineering
â”‚   â”œâ”€â”€ harness.py
â”‚   â””â”€â”€ results/
â””â”€â”€ custom/                 # FilAgent-specific benchmarks
    â”œâ”€â”€ compliance/         # Governance & legal compliance
    â”‚   â”œâ”€â”€ harness.py
    â”‚   â””â”€â”€ tasks/
    â”œâ”€â”€ htn_planning/       # HTN planning capabilities
    â”‚   â”œâ”€â”€ harness.py
    â”‚   â””â”€â”€ tasks/
    â””â”€â”€ tool_orchestration/ # Multi-tool coordination
        â”œâ”€â”€ harness.py
        â””â”€â”€ tasks/
```

## Available Benchmarks

### Industry Standard

- **HumanEval** (`eval/humaneval.py`): Code generation benchmark
- **MBPP** (`eval/mbpp.py`): Mostly Basic Python Problems
- **SWE-bench** (`swe_bench/`): Real-world GitHub issues

### FilAgent Custom

- **Compliance** (`custom/compliance/`): Decision Records, PII, WORM, RBAC
- **HTN Planning** (`custom/htn_planning/`): Task decomposition, parallel execution
- **Tool Orchestration** (`custom/tool_orchestration/`): Multi-tool coordination

## Quick Start

```bash
# Run all benchmarks
python eval/runner.py --all

# Run specific benchmark
python eval/runner.py --benchmark compliance

# Run custom benchmarks only
python eval/runner.py --custom-only
```

## Documentation

See `/docs/BENCHMARKS.md` for comprehensive documentation.

## Targets

| Benchmark | Target | Status |
|-----------|--------|--------|
| HumanEval | â‰¥65% pass@1 | ðŸŽ¯ |
| MBPP | â‰¥60% pass@1 | ðŸŽ¯ |
| SWE-bench | â‰¥30% resolution | ðŸŽ¯ |
| Compliance | 100% | âœ… CRITICAL |
| HTN Planning | â‰¥90% | ðŸŽ¯ |
| Tool Orchestration | â‰¥80% | ðŸŽ¯ |

## Adding Custom Benchmarks

1. Create harness class extending `BenchmarkHarness`
2. Implement `load_tasks()` and `evaluate()`
3. Register in `eval/runner.py`
4. Add tests in `tests/test_benchmarks.py`

See existing custom benchmarks for examples.
