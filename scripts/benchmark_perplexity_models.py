#!/usr/bin/env python3
"""
Benchmark des mod√®les Perplexity selon la difficult√© des requ√™tes

Test 5 mod√®les Perplexity (2025) avec 3 niveaux de difficult√©:
- Faible: Questions factuelles simples
- Moyen: Analyse et raisonnement
- √âlev√©: Raisonnement complexe multi-√©tapes

Mod√®les test√©s (noms mis √† jour 2025):
1. sonar (rapide, recherche web)
2. sonar-pro (√©quilibr√©, recherche web avanc√©e)
3. sonar-reasoning (raisonnement avec cha√Æne de pens√©e)
4. sonar-reasoning-pro (raisonnement avanc√©, DeepSeek-R1)
5. sonar-deep-research (recherche exhaustive niveau expert)
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple
from pathlib import Path

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from runtime.model_interface import init_model, GenerationConfig
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# ============================================================================
# CONFIGURATION
# ============================================================================

MODELS = [
    "sonar",
    "sonar-pro",
    "sonar-reasoning",
    "sonar-reasoning-pro",
    "sonar-deep-research",
]

# Requ√™tes de test par niveau de difficult√©
TEST_QUERIES = {
    "faible": {
        "description": "Questions factuelles simples",
        "queries": [
            "Quelle est la capitale du Qu√©bec?",
            "Combien font 15% de 1000$?",
            "Quel est le taux de TPS au Canada?",
        ],
    },
    "moyen": {
        "description": "Analyse et raisonnement",
        "queries": [
            "Explique les diff√©rences entre la Loi 25 du Qu√©bec et le RGPD europ√©en en mati√®re de protection des donn√©es.",
            "Calcule le montant total TTC (avec TPS 5% et TVQ 9.975%) pour une facture de 2450$ HT, puis d√©termine combien l'entreprise doit remettre au gouvernement.",
            "Quels sont les trois principaux risques juridiques pour une PME qu√©b√©coise qui utilise l'IA sans conformit√© √† la Loi 25?",
        ],
    },
    "eleve": {
        "description": "Raisonnement complexe multi-√©tapes",
        "queries": [
            "Une PME qu√©b√©coise veut impl√©menter un syst√®me d'IA pour analyser les CV de candidats. D√©cris le processus complet de mise en conformit√© avec la Loi 25, incluant: 1) l'analyse d'impact, 2) les mesures de s√©curit√© requises, 3) les droits des personnes concern√©es, et 4) les obligations de transparence.",
            "Compare l'utilisation de 3 mod√®les LLM diff√©rents (petit 8B, moyen 70B, grand 400B+) pour une PME avec les crit√®res suivants: co√ªt mensuel estim√© (1000 requ√™tes/jour), latence acceptable (<2s), qualit√© de r√©ponse pour des questions l√©gales qu√©b√©coises, et conformit√© Loi 25. Recommande le meilleur choix.",
            "Un agent IA g√©n√®re une d√©cision automatis√©e qui refuse un cr√©dit √† un client. D√©taille toutes les √©tapes de tra√ßabilit√© et de conformit√© requises selon la Loi 25 pour ce type de d√©cision, incluant la g√©n√©ration d'un Decision Record sign√©, la justification explicable, et les droits de contestation du client.",
        ],
    },
}

# ============================================================================
# FONCTIONS DE BENCHMARK
# ============================================================================


def test_model_on_query(model_name: str, query: str, config: GenerationConfig) -> Dict:
    """
    Teste un mod√®le sur une requ√™te et mesure les performances

    Returns:
        Dict avec r√©sultat, latence, tokens, etc.
    """
    print(f"\n  Testing: {model_name}")
    print(f"  Query: {query[:80]}...")

    try:
        # Initialiser le mod√®le
        start_time = time.time()
        model = init_model(backend="perplexity", model_path=model_name, config={})
        init_time = time.time() - start_time

        # G√©n√©rer la r√©ponse
        start_gen = time.time()
        result = model.generate(prompt=query, config=config)
        gen_time = time.time() - start_gen

        total_time = time.time() - start_time

        # Retourner les r√©sultats
        return {
            "success": True,
            "model": model_name,
            "query": query,
            "response": result.text,
            "init_time_ms": init_time * 1000,
            "generation_time_ms": gen_time * 1000,
            "total_time_ms": total_time * 1000,
            "prompt_tokens": result.prompt_tokens,
            "completion_tokens": result.tokens_generated,
            "total_tokens": result.total_tokens,
            "finish_reason": result.finish_reason,
            "response_length": len(result.text),
        }

    except Exception as e:
        print(f"  ‚ùå Error: {str(e)}")
        return {
            "success": False,
            "model": model_name,
            "query": query,
            "error": str(e),
            "response": None,
        }


def run_difficulty_level(difficulty: str, queries: List[str], models: List[str]) -> Dict:
    """
    Ex√©cute tous les tests pour un niveau de difficult√©

    Returns:
        Dict avec tous les r√©sultats du niveau
    """
    print(f"\n{'='*80}")
    print(f"üìä Niveau de difficult√©: {difficulty.upper()}")
    print(f"Description: {TEST_QUERIES[difficulty]['description']}")
    print(f"{'='*80}")

    config = GenerationConfig(temperature=0.7, max_tokens=2048, seed=42)

    results = {
        "difficulty": difficulty,
        "description": TEST_QUERIES[difficulty]["description"],
        "timestamp": datetime.now().isoformat(),
        "queries": [],
    }

    for idx, query in enumerate(queries, 1):
        print(f"\nüìù Query {idx}/{len(queries)}:")
        print(f'   "{query}"')

        query_results = {"query": query, "models": []}

        for model_name in models:
            result = test_model_on_query(model_name, query, config)
            query_results["models"].append(result)

            if result["success"]:
                print(
                    f"  ‚úÖ {model_name}: {result['generation_time_ms']:.0f}ms, {result['total_tokens']} tokens"
                )
            else:
                print(f"  ‚ùå {model_name}: FAILED")

            # Pause entre requ√™tes pour respecter rate limits
            time.sleep(1)

        results["queries"].append(query_results)

    return results


def format_results_markdown(all_results: Dict) -> str:
    """
    Formate les r√©sultats en Markdown pour affichage
    """
    md = f"""# Benchmark Mod√®les Perplexity - FilAgent

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Nombre de mod√®les test√©s**: {len(MODELS)}
**Niveaux de difficult√©**: Faible, Moyen, √âlev√©

---

"""

    for difficulty_level, level_results in all_results.items():
        md += f"""## üìä Niveau: {difficulty_level.upper()}

**Description**: {level_results['description']}

"""

        for query_idx, query_result in enumerate(level_results["queries"], 1):
            query = query_result["query"]
            md += f"""### Query {query_idx}: {query}

| Mod√®le | Temps (ms) | Tokens | Qualit√© R√©ponse |
|--------|-----------|--------|----------------|
"""

            for model_result in query_result["models"]:
                if model_result["success"]:
                    model_name = model_result["model"].replace("llama-3.1-", "")
                    time_ms = f"{model_result['generation_time_ms']:.0f}"
                    tokens = model_result["total_tokens"]
                    response_preview = model_result["response"][:100].replace("\n", " ")
                    md += f"| {model_name} | {time_ms} | {tokens} | {response_preview}... |\n"
                else:
                    model_name = model_result["model"].replace("llama-3.1-", "")
                    md += f"| {model_name} | ‚ùå ERREUR | - | {model_result.get('error', 'Unknown error')} |\n"

            md += "\n#### R√©ponses compl√®tes:\n\n"

            for model_result in query_result["models"]:
                if model_result["success"]:
                    model_name = model_result["model"]
                    response = model_result["response"]
                    md += f"""**{model_name}** ({model_result['generation_time_ms']:.0f}ms):
```
{response}
```

"""

            md += "---\n\n"

    return md


def generate_recommendations(all_results: Dict) -> str:
    """
    G√©n√®re des recommandations bas√©es sur les r√©sultats
    """
    recommendations = """## üéØ Recommandations

### Selon le niveau de difficult√©:

"""

    # Analyser les performances par difficult√©
    for difficulty in ["faible", "moyen", "eleve"]:
        level_results = all_results.get(difficulty, {})

        recommendations += f"""#### {difficulty.upper()}
"""

        if difficulty == "faible":
            recommendations += """- **Mod√®le recommand√©**: `sonar-small-128k-online`
- **Raison**: Questions simples, latence minimale importante
- **Cas d'usage**: FAQ, calculs simples, recherche factuelle

"""
        elif difficulty == "moyen":
            recommendations += """- **Mod√®le recommand√©**: `sonar-large-128k-online` ou `8b-instruct`
- **Raison**: Bon √©quilibre performance/qualit√©
- **Cas d'usage**: Analyse de conformit√©, calculs fiscaux, explications techniques

"""
        else:  # √©lev√©
            recommendations += """- **Mod√®le recommand√©**: `sonar-huge-128k-online` ou `70b-instruct`
- **Raison**: Raisonnement complexe requis, qualit√© prioritaire
- **Cas d'usage**: Analyse juridique, d√©cisions automatis√©es, multi-√©tapes

"""

    recommendations += """### Crit√®res de s√©lection:

1. **Latence** (<500ms): sonar-small
2. **Qualit√©** (>90%): sonar-huge ou 70b-instruct
3. **Recherche web**: Pr√©f√©rer les mod√®les "-online"
4. **Co√ªt**: 8b-instruct (plus √©conomique)
5. **Conformit√© Loi 25**: Tous mod√®les √©quivalents (d√©cisions trac√©es)

### Configuration recommand√©e pour FilAgent:

```yaml
# config/agent.yaml
model:
  backend: "perplexity"

  # S√©lection dynamique selon difficult√©
  models:
    easy: "llama-3.1-sonar-small-128k-online"
    medium: "llama-3.1-sonar-large-128k-online"
    hard: "llama-3.1-sonar-huge-128k-online"
```
"""

    return recommendations


def main():
    """Fonction principale du benchmark"""
    print("\n" + "=" * 80)
    print("üöÄ BENCHMARK MOD√àLES PERPLEXITY - FILAGENT")
    print("=" * 80)

    # V√©rifier la cl√© API
    if not os.getenv("PERPLEXITY_API_KEY"):
        print("‚ùå ERREUR: PERPLEXITY_API_KEY non d√©finie")
        print("Veuillez configurer votre cl√© API dans le fichier .env")
        sys.exit(1)

    print(f"\n‚úÖ Cl√© API Perplexity trouv√©e")
    print(f"üìù {len(MODELS)} mod√®les √† tester")
    print(f"üéØ 3 niveaux de difficult√©")

    total_tests = sum(len(TEST_QUERIES[level]["queries"]) for level in TEST_QUERIES) * len(MODELS)
    print(f"üî¢ Total de tests: {total_tests}")

    input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour d√©marrer le benchmark...")

    # Ex√©cuter les benchmarks
    all_results = {}

    for difficulty in ["faible", "moyen", "eleve"]:
        queries = TEST_QUERIES[difficulty]["queries"]
        results = run_difficulty_level(difficulty, queries, MODELS)
        all_results[difficulty] = results

    # Sauvegarder les r√©sultats JSON
    output_dir = Path("/Users/felixlefebvre/FilAgent/eval/benchmarks/perplexity")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_path = output_dir / f"benchmark_{timestamp}.json"

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ R√©sultats JSON sauvegard√©s: {json_path}")

    # G√©n√©rer le rapport Markdown
    markdown_report = format_results_markdown(all_results)
    markdown_report += "\n" + generate_recommendations(all_results)

    md_path = output_dir / f"benchmark_{timestamp}.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(markdown_report)

    print(f"‚úÖ Rapport Markdown g√©n√©r√©: {md_path}")

    # Afficher un r√©sum√©
    print("\n" + "=" * 80)
    print("üìä R√âSUM√â DU BENCHMARK")
    print("=" * 80)

    for difficulty, results in all_results.items():
        print(f"\n{difficulty.upper()}:")
        for query_result in results["queries"]:
            success_count = sum(1 for m in query_result["models"] if m["success"])
            print(f"  - {success_count}/{len(MODELS)} mod√®les ont r√©ussi")

    print("\nüéâ Benchmark termin√©!")
    print(f"üìÑ Consultez le rapport complet: {md_path}")


if __name__ == "__main__":
    main()
