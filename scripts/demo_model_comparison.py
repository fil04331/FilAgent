#!/usr/bin/env python3
"""
D√©monstration rapide de comparaison des mod√®les Perplexity

Teste 1 question par niveau de difficult√© avec 3 mod√®les repr√©sentatifs
"""

import os
import sys
import time
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))

from runtime.model_interface import init_model, GenerationConfig
from dotenv import load_dotenv

load_dotenv()

# Mod√®les √† tester (repr√©sentatifs) - Noms mis √† jour 2025
DEMO_MODELS = [
    ("sonar", "sonar", "Rapide"),
    ("sonar-pro", "sonar-pro", "√âquilibr√©"),
    ("sonar-reasoning", "sonar-reasoning", "Raisonnement"),
]

# Une question par niveau
DEMO_QUERIES = {
    "faible": "Quelle est la capitale du Qu√©bec?",
    "moyen": "Explique les diff√©rences entre la Loi 25 du Qu√©bec et le RGPD europ√©en en 3 points.",
    "eleve": "Une PME qu√©b√©coise veut impl√©menter un syst√®me d'IA pour analyser les CV. Quelles sont les 3 principales obligations de conformit√© avec la Loi 25?",
}


def test_query_with_model(query: str, model_name: str, model_full_name: str) -> dict:
    """Teste une requ√™te avec un mod√®le"""
    print(f"\n  üîÑ Test avec {model_name}...")

    try:
        start_time = time.time()

        # Charger le mod√®le
        model = init_model(backend="perplexity", model_path=model_full_name, config={})

        # G√©n√©rer
        config = GenerationConfig(temperature=0.7, max_tokens=2048, seed=42)
        result = model.generate(prompt=query, config=config)

        elapsed = (time.time() - start_time) * 1000

        print(f"  ‚úÖ R√©ponse en {elapsed:.0f}ms - {result.total_tokens} tokens")

        return {
            "success": True,
            "model": model_name,
            "response": result.text,
            "time_ms": elapsed,
            "tokens": result.total_tokens,
        }

    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return {"success": False, "model": model_name, "error": str(e)}


def main():
    print("\n" + "=" * 80)
    print("üéØ D√âMONSTRATION COMPARAISON MOD√àLES PERPLEXITY")
    print("=" * 80)

    # V√©rifier cl√© API
    if not os.getenv("PERPLEXITY_API_KEY"):
        print("\n‚ùå ERREUR: PERPLEXITY_API_KEY non d√©finie")
        print("Configurez votre cl√© API dans le fichier .env\n")
        return

    print("\n‚úÖ Cl√© API Perplexity d√©tect√©e")
    print(f"üìù 3 niveaux de difficult√© √ó 3 mod√®les = 9 tests\n")

    all_results = {}

    # Tester chaque niveau
    for difficulty, query in DEMO_QUERIES.items():
        print("\n" + "=" * 80)
        print(f"üìä NIVEAU: {difficulty.upper()}")
        print("=" * 80)
        print(f"\n‚ùì Question: {query}\n")

        results = []

        for model_key, model_full, model_desc in DEMO_MODELS:
            result = test_query_with_model(query, model_desc, model_full)
            results.append(result)
            time.sleep(0.5)  # Pause entre requ√™tes

        all_results[difficulty] = {"query": query, "results": results}

    # Afficher le rapport
    print("\n\n" + "=" * 80)
    print("üìã RAPPORT DE COMPARAISON")
    print("=" * 80)

    for difficulty, data in all_results.items():
        print(f"\n\n## {difficulty.upper()}\n")
        print(f"**Question**: {data['query']}\n")

        for result in data["results"]:
            if result["success"]:
                print(f"\n### {result['model']}")
                print(f"‚è±Ô∏è  Temps: {result['time_ms']:.0f}ms")
                print(f"üéØ Tokens: {result['tokens']}")
                print(f"\n**R√©ponse**:")
                print(f"```\n{result['response']}\n```")
                print("-" * 80)
            else:
                print(f"\n### {result['model']}")
                print(f"‚ùå Erreur: {result['error']}")
                print("-" * 80)

    # G√©n√©rer les recommandations
    print("\n\n" + "=" * 80)
    print("üéØ RECOMMANDATIONS")
    print("=" * 80)

    print("""
### FAIBLE difficult√©
- **Recommand√©**: Sonar Small (Rapide)
- **Raison**: Questions simples, latence minimale
- **Cas**: FAQ, calculs, recherche factuelle

### MOYEN difficult√©
- **Recommand√©**: Sonar Large (√âquilibr√©)
- **Raison**: Bon compromis vitesse/qualit√©
- **Cas**: Analyse conformit√©, explications techniques

### √âLEV√â difficult√©
- **Recommand√©**: Sonar Huge (Pr√©cis)
- **Raison**: Raisonnement complexe requis
- **Cas**: Analyse juridique, d√©cisions automatis√©es

### Configuration FilAgent recommand√©e:

```yaml
# config/agent.yaml
model:
  backend: "perplexity"

  # S√©lection dynamique selon difficult√©
  models:
    easy: "llama-3.1-sonar-small-128k-online"
    medium: "llama-3.1-sonar-large-128k-online"
    hard: "llama-3.1-sonar-huge-128k-online"
```
    """)

    print("\n‚úÖ D√©monstration termin√©e!")
    print("\nüí° Pour lancer l'interface interactive:")
    print("   python gradio_app_model_selector.py")
    print("\nüí° Pour le benchmark complet:")
    print("   python scripts/benchmark_perplexity_models.py\n")


if __name__ == "__main__":
    main()
