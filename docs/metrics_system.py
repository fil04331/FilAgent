# -*- coding: utf-8 -*-
"""Metrics System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gSM1uHIX4_wavAYfPotmWSKNIuAMkFHE
"""

import json
import shutil
import tempfile
from datetime import datetime, date
from pathlib import Path
from typing import Dict, List, Optional
from pydantic import BaseModel, Field, StrictInt, StrictFloat, field_validator, ConfigDict

# --- CONSTANTES & CONFIGURATION ---
METRICS_FILE = Path("data/metrics_storage.json")
BACKUP_COUNT = 5

class DailyStats(BaseModel):
    """
    Modèle représentant les métriques agrégées pour une journée spécifique.
    Strictly typed pour éviter les dérives de données.
    """
    date_ref: str = Field(..., description="Date de référence au format ISO YYYY-MM-DD")
    total_operations: StrictInt = Field(default=0, ge=0)
    total_errors: StrictInt = Field(default=0, ge=0)
    total_tokens_used: StrictInt = Field(default=0, ge=0)
    avg_latency_ms: StrictFloat = Field(default=0.0, ge=0.0)

    # Configuration Pydantic V2 stricte
    model_config = ConfigDict(frozen=False, validate_assignment=True)

class SessionStats(BaseModel):
    """
    Métriques volatiles de la session en cours.
    """
    start_time: datetime = Field(default_factory=datetime.now)
    uptime_seconds: float = Field(default=0.0)
    active_operations: StrictInt = Field(default=0)

class GlobalMetrics(BaseModel):
    """
    Agrégat racine pour la persistance.
    Contient l'historique et l'état actuel.
    """
    last_updated: datetime = Field(default_factory=datetime.now)
    version: str = Field(default="1.0.0")
    # Mapping date string -> DailyStats pour accès O(1)
    history: Dict[str, DailyStats] = Field(default_factory=dict)

    @field_validator('history')
    @classmethod
    def validate_history_keys(cls, v: Dict[str, DailyStats]) -> Dict[str, DailyStats]:
        """Vérifie que les clés correspondent au format de date."""
        for key in v.keys():
            try:
                datetime.strptime(key, "%Y-%m-%d")
            except ValueError:
                raise ValueError(f"Clé d'historique invalide: {key}. Format attendu YYYY-MM-DD")
        return v

class MetricsManager:
    """
    Gestionnaire de métriques thread-safe avec persistance atomique.
    Respecte le principe SOLID (Single Responsibility).
    """

    def __init__(self, storage_path: Path = METRICS_FILE):
        self.storage_path = storage_path
        self.session = SessionStats()
        self.data = self._load_or_init()

    def _load_or_init(self) -> GlobalMetrics:
        """Charge les données existantes ou initialise une structure vide."""
        if not self.storage_path.exists():
            return GlobalMetrics()

        try:
            with open(self.storage_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            return GlobalMetrics(**raw_data)
        except (json.JSONDecodeError, ValueError) as e:
            # Audit First: On ne supprime pas silencieusement les données corrompues.
            # On les déplace pour analyse ultérieure.
            backup_path = self.storage_path.with_suffix(f".corrupt.{datetime.now().timestamp()}.json")
            if self.storage_path.exists():
                shutil.move(str(self.storage_path), str(backup_path))
            print(f"[WARN] Metrics corrupted. Moved to {backup_path}. Error: {e}")
            return GlobalMetrics()

    def record_operation(self, tokens: int, latency_ms: float, is_error: bool = False) -> None:
        """
        Enregistre une opération atomique.
        Met à jour la session courante ET l'historique journalier.
        """
        today_key = date.today().isoformat()

        # 1. Mise à jour ou création du bucket journalier
        if today_key not in self.data.history:
            self.data.history[today_key] = DailyStats(date_ref=today_key)

        daily = self.data.history[today_key]

        # 2. Mise à jour des compteurs (Logique métier)
        daily.total_operations += 1
        daily.total_tokens_used += tokens
        if is_error:
            daily.total_errors += 1

        # Moyenne glissante simple pour la latence
        # Note: Pour une précision statistique parfaite, on stockerait sum_latency et count séparément.
        # Ici simplifié pour l'exemple.
        n = daily.total_operations
        daily.avg_latency_ms = ((daily.avg_latency_ms * (n - 1)) + latency_ms) / n

        self.data.last_updated = datetime.now()

        # 3. Persistance (Devrait idéalement être asynchrone ou périodique pour la perf)
        self._persist()

    def _persist(self) -> None:
        """
        Écriture atomique sur disque pour éviter la corruption en cas de crash pendant l'écriture.
        """
        # Création d'un fichier temporaire dans le même dossier
        temp_file = tempfile.NamedTemporaryFile(
            mode='w',
            dir=self.storage_path.parent,
            delete=False,
            encoding='utf-8'
        )

        try:
            # Sérialisation Pydantic
            json_str = self.data.model_dump_json(indent=2)
            temp_file.write(json_str)
            temp_file.flush()
            temp_file.close()

            # Remplacement atomique
            shutil.move(temp_file.name, str(self.storage_path))
        except Exception as e:
            Path(temp_file.name).unlink(missing_ok=True)
            raise RuntimeError(f"Critical: Failed to persist metrics. {e}")

    def get_stats_summary(self) -> Dict[str, Any]: # type: ignore - Any used for broad output dict
        """Retourne un résumé pour l'API/Dashboard."""
        today = date.today().isoformat()
        today_stats = self.data.history.get(today, DailyStats(date_ref=today))

        return {
            "status": "healthy",
            "last_updated": self.data.last_updated,
            "session_uptime": (datetime.now() - self.session.start_time).seconds,
            "today": today_stats.model_dump(),
            "historical_days_tracked": len(self.data.history)
        }