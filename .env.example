# FilAgent Environment Variables Example
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Backend Configuration
# =============================================================================

# Backend selection: Choose ONE of the two options below
#
# Option 1: "perplexity" (RECOMMENDED for quick start)
#   - No model download required
#   - Real-time web search (Sonar models)
#   - Fast setup (< 5 minutes)
#   - Requires API key (get from https://www.perplexity.ai/settings/api)
#
# Option 2: "llama.cpp" (for maximum privacy)
#   - 100% local inference
#   - No Internet dependency
#   - Requires model download (~4-8 GB)
#   - See models/weights/README.md for setup
#
# Current default: perplexity (matches production configuration)
LLM_BACKEND=perplexity

# -----------------------------------------------------------------------------
# Configuration for llama.cpp (local model) - ONLY if LLM_BACKEND=llama.cpp
# -----------------------------------------------------------------------------
# BEFORE using local model, download GGUF file:
#   cd models/weights
#   wget https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF/resolve/main/llama-3-8b-instruct.Q4_K_M.gguf -O base.gguf
# See models/weights/README.md for detailed instructions

MODEL_PATH=models/weights/base.gguf
CONTEXT_SIZE=4096              # Token context window (2048-8192)
N_GPU_LAYERS=35               # Layers on GPU (35 for GPU, 0 for CPU-only)

# -----------------------------------------------------------------------------
# Configuration for Perplexity API - ONLY if LLM_BACKEND=perplexity
# -----------------------------------------------------------------------------
# Get your API key from: https://www.perplexity.ai/settings/api
# Documentation: docs/PERPLEXITY_INTEGRATION.md

PERPLEXITY_API_KEY=pplx-your-api-key-here
PERPLEXITY_MODEL=llama-3.1-sonar-large-128k-online

# Available Perplexity models:
#
# Sonar models (with real-time web search - RECOMMENDED):
#   - llama-3.1-sonar-small-128k-online   (Fast, cost-effective, ~$1/M tokens)
#   - llama-3.1-sonar-large-128k-online   (Balanced - DEFAULT, ~$1/M tokens)
#   - llama-3.1-sonar-huge-128k-online    (Best quality, ~$5/M tokens)
#
# Chat models (no web search, faster response):
#   - llama-3.1-8b-instruct               (Fast, lightweight)
#   - llama-3.1-70b-instruct              (High quality, complex reasoning)

# =============================================================================
# Server Configuration
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# =============================================================================
# Database Configuration
# =============================================================================
DATABASE_PATH=data/filagent.db

# =============================================================================
# Security
# =============================================================================
# Generate with: openssl rand -hex 32
SECRET_KEY=your-secret-key-here

# =============================================================================
# Logging & Monitoring
# =============================================================================
LOG_LEVEL=INFO
ENABLE_PROMETHEUS=true

# =============================================================================
# Compliance
# =============================================================================
ENABLE_AUDIT_TRAIL=true
ENABLE_PII_REDACTION=true
ENABLE_WORM_LOGGING=true
