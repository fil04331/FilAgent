# üìã PLAN D'ACTION D√âTAILL√â - JOUR 1 (8 D√©cembre)

## üöÄ PHASE 1: FIXES CRITIQUES (45 minutes)

### Fix #1: TaskNode Import Error (15 min)

**Fichier √† corriger:** `tests/test_performance.py`

**√âtapes:**
1. Lire le fichier et identifier toutes les r√©f√©rences √† `TaskNode`
2. Les remplacer par `Task` 
3. Valider que la syntaxe reste correcte

**Code actuel (MAUVAIS):**
```python
from planner.task_graph import TaskGraph, TaskNode
# puis l'utilisation
task_node = TaskNode(...)
```

**Code apr√®s correction (BON):**
```python
from planner.task_graph import TaskGraph, Task
# puis l'utilisation  
task = Task(...)
```

---

### Fix #2: Pin Dependencies (10 min)

**Fichier √† corriger:** `requirements.txt`

**Chercher la ligne:**
```
datasets
```

**Remplacer par:**
```
datasets~=2.15.0
```

---

### Fix #3: Stricter ComplianceGuardian Tests (20 min)

**Fichier √† corriger:** `tests/test_agent.py`

**Chercher les lignes:**
```python
assert agent.compliance_guardian is None or isinstance(agent.compliance_guardian, ComplianceGuardian)
```

**Remplacer par:**
```python
assert hasattr(agent, 'compliance_guardian')
assert isinstance(agent.compliance_guardian, ComplianceGuardian)
```

---

## üîß PHASE 2: TESTS DE COMPLIANCE - IMPL√âMENTATIONS (2-3 heures)

### Fix #4: Impl√©menter _evaluate_provenance()

**Fichier:** `eval/compliance_benchmark.py`

**Remplacement complet de la fonction:**

```python
def _evaluate_provenance(self) -> bool:
    """
    Valider que les m√©tadonn√©es de provenance W3C PROV-JSON sont pr√©sentes.
    
    Crit√®res:
    - Chaque action a un hash Merkle
    - Trace compl√®te de la d√©cision enregistr√©e
    - Signatures num√©riques valides
    - Immuabilit√© garantie
    """
    try:
        # 1. V√©rifier que les logs existent
        if not hasattr(self.agent, 'audit_trail'):
            logger.error("Agent missing audit_trail")
            return False
        
        # 2. V√©rifier la structure WORM
        for log_entry in self.agent.audit_trail:
            # Chaque entr√©e doit avoir:
            # - id (unique)
            # - timestamp (ISO 8601)
            # - action (type d'action)
            # - hash (Merkle tree hash)
            # - previous_hash (cha√Æne d'int√©grit√©)
            # - signature (Ed25519)
            
            required_fields = ['id', 'timestamp', 'action', 'hash', 'previous_hash', 'signature']
            if not all(field in log_entry for field in required_fields):
                logger.error(f"Missing provenance fields in entry: {log_entry}")
                return False
            
            # 3. Valider la cha√Æne de hachage
            if log_entry['hash'] is None:
                logger.error(f"Hash is None for entry {log_entry['id']}")
                return False
            
            # 4. Valider la signature (simul√© ici, faut v√©rifier avec cl√© publique r√©elle)
            if not self._verify_signature(log_entry):
                logger.error(f"Signature verification failed for entry {log_entry['id']}")
                return False
        
        logger.info("‚úÖ Provenance validation passed")
        return True
        
    except Exception as e:
        logger.error(f"Provenance evaluation failed: {e}")
        return False

def _verify_signature(self, log_entry: dict) -> bool:
    """Valider la signature Ed25519 d'une entr√©e de log."""
    try:
        # TODO: Impl√©menter avec crypto r√©elle (nacl.signing)
        # Pour maintenant, v√©rifier la pr√©sence de la signature
        signature = log_entry.get('signature')
        return signature is not None and len(signature) == 128  # 64 bytes en hex
    except Exception:
        return False
```

---

### Fix #5: Impl√©menter _evaluate_audit_trail()

**Fichier:** `eval/compliance_benchmark.py`

```python
def _evaluate_audit_trail(self) -> bool:
    """
    Valider le journal WORM (Write-Once-Read-Many).
    
    Crit√®res:
    - Immuabilit√© des logs confirm√©e
    - Cha√Æne de hachage valide (pas de bris)
    - Ordre chronologique respect√©
    - Format conforme JSON Lines
    """
    try:
        audit_trail = self.agent.audit_trail
        
        # 1. V√©rifier que c'est une liste non-vide
        if not audit_trail or len(audit_trail) == 0:
            logger.error("Audit trail is empty")
            return False
        
        # 2. Valider l'ordre chronologique
        previous_timestamp = None
        previous_hash = None
        
        for i, entry in enumerate(audit_trail):
            # Timestamps doivent √™tre croissants
            current_timestamp = entry.get('timestamp')
            
            if previous_timestamp and current_timestamp < previous_timestamp:
                logger.error(f"Chronological order violation at entry {i}")
                return False
            
            # 3. Valider la cha√Æne de hachage (int√©grit√©)
            current_hash = entry.get('hash')
            expected_previous_hash = entry.get('previous_hash')
            
            if i > 0 and expected_previous_hash != previous_hash:
                logger.error(f"Hash chain broken at entry {i}")
                logger.error(f"  Expected: {previous_hash}")
                logger.error(f"  Got: {expected_previous_hash}")
                return False
            
            # 4. V√©rifier la structure JSON Lines
            required_fields = ['id', 'timestamp', 'action', 'hash', 'previous_hash']
            if not all(field in entry for field in required_fields):
                logger.error(f"Missing field in audit trail entry {i}: {entry.keys()}")
                return False
            
            previous_timestamp = current_timestamp
            previous_hash = current_hash
        
        logger.info(f"‚úÖ Audit trail validation passed ({len(audit_trail)} entries)")
        return True
        
    except Exception as e:
        logger.error(f"Audit trail evaluation failed: {e}")
        return False
```

---

### Fix #6: Impl√©menter _evaluate_retention()

**Fichier:** `eval/compliance_benchmark.py`

```python
def _evaluate_retention(self) -> bool:
    """
    Valider que les politiques de r√©tention sont appliqu√©es correctement.
    
    Crit√®res:
    - Donn√©es personnelles supprim√©es selon la politique
    - Logs conserv√©s selon les r√®gles de conformit√©
    - Minimisation des donn√©es respect√©e
    - TTL appliqu√© correctement
    """
    try:
        # 1. V√©rifier la configuration de r√©tention
        retention_config = self.agent.config.get('retention', {})
        
        if not retention_config:
            logger.warning("No retention policy configured")
            return False
        
        # 2. Valider les TTLs (Time To Live)
        required_ttl_keys = ['pii_ttl_days', 'logs_ttl_days', 'cache_ttl_days']
        for key in required_ttl_keys:
            if key not in retention_config:
                logger.error(f"Missing retention config: {key}")
                return False
            
            ttl_value = retention_config[key]
            if not isinstance(ttl_value, int) or ttl_value <= 0:
                logger.error(f"Invalid {key}: {ttl_value}")
                return False
        
        # 3. V√©rifier que les donn√©es personnelles ont un TTL < logs
        pii_ttl = retention_config['pii_ttl_days']
        logs_ttl = retention_config['logs_ttl_days']
        
        if pii_ttl >= logs_ttl:
            logger.error("PII TTL should be less than logs TTL (minimize retention)")
            return False
        
        # 4. Valider l'application des politiques
        if hasattr(self.agent, 'memory'):
            # V√©rifier que pas de PII stock√©e au-del√† du TTL
            pii_entries = [e for e in self.agent.memory.episodic_store if e.contains_pii]
            
            for entry in pii_entries:
                age_days = (datetime.now() - entry.created_at).days
                if age_days > pii_ttl:
                    logger.error(f"PII entry age ({age_days}d) exceeds TTL ({pii_ttl}d)")
                    return False
        
        logger.info("‚úÖ Retention policies validation passed")
        return True
        
    except Exception as e:
        logger.error(f"Retention evaluation failed: {e}")
        return False
```

---

### Fix #7: Impl√©menter _evaluate_multi_step()

**Fichier:** `eval/compliance_benchmark.py`

```python
def _evaluate_multi_step(self) -> bool:
    """
    Valider que les tasks multi-√©tapes sont d√©compos√©es et ex√©cut√©es correctement.
    
    Crit√®res:
    - D√©composition HTN (Hierarchical Task Network) correcte
    - D√©pendances entre t√¢ches respect√©es
    - Gestion des erreurs dans les subtasks
    - Rollback en cas d'√©chec
    """
    try:
        # Cr√©er une task complexe pour tester
        test_task = {
            "name": "complex_workflow",
            "type": "sequential",
            "subtasks": [
                {"name": "step1", "type": "analyze"},
                {"name": "step2", "type": "process"},
                {"name": "step3", "type": "validate"}
            ],
            "dependencies": {
                "step2": ["step1"],  # step2 d√©pend de step1
                "step3": ["step2"]   # step3 d√©pend de step2
            }
        }
        
        # 1. V√©rifier la d√©composition
        if not hasattr(self.agent.planner, 'decompose'):
            logger.error("Planner missing decompose method")
            return False
        
        decomposed = self.agent.planner.decompose(test_task)
        
        if not decomposed:
            logger.error("Task decomposition returned empty")
            return False
        
        # 2. V√©rifier que les d√©pendances sont respect√©es
        execution_order = [t.name for t in decomposed]
        
        for task_name, deps in test_task['dependencies'].items():
            for dep in deps:
                if execution_order.index(dep) >= execution_order.index(task_name):
                    logger.error(f"Dependency order violation: {task_name} before {dep}")
                    return False
        
        # 3. Ex√©cuter et v√©rifier la gestion d'erreurs
        result = self.agent.planner.execute(decomposed)
        
        if not result:
            logger.error("Task execution failed")
            return False
        
        logger.info("‚úÖ Multi-step task validation passed")
        return True
        
    except Exception as e:
        logger.error(f"Multi-step evaluation failed: {e}")
        return False
```

---

## üìä PHASE 3: CONFIGURATION √âVALUATION DATA-DRIVEN (1 heure)

### Fix #8: Rendre _check_targets() data-driven

**Fichier:** `eval/compliance_benchmark.py`

**Remplacer la logique hardcod√©e par:**

```python
from typing import List
from dataclasses import dataclass

@dataclass
class EvaluationTarget:
    """Cible d'√©valuation configurable."""
    benchmark: str
    metric: str
    operator: str  # ">=", ">", "==", "<=", "<"
    value: float
    description: str = ""

class EvaluationTargetLoader:
    """Charger les cibles d'√©valuation depuis la configuration."""
    
    @staticmethod
    def load_targets(config_path: str = "config/eval_targets.yaml") -> List[EvaluationTarget]:
        """
        Charger les targets d'√©valuation depuis YAML.
        
        Format YAML attendu:
        ```yaml
        targets:
          - benchmark: humaneval
            metric: pass_rate
            operator: ">="
            value: 65
            description: "HumanEval pass@1 baseline"
          
          - benchmark: compliance
            metric: provenance_integrity
            operator: "=="
            value: 100
            description: "Provenance tests must pass 100%"
        ```
        """
        try:
            if not os.path.exists(config_path):
                raise FileNotFoundError(f"Config file not found: {config_path}")
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            if not config or 'targets' not in config:
                raise ValueError("Config must contain 'targets' key")
            
            targets = []
            for target_dict in config['targets']:
                # Validation
                required_fields = ['benchmark', 'metric', 'operator', 'value']
                missing = [f for f in required_fields if f not in target_dict]
                
                if missing:
                    raise ValueError(f"Missing fields in target: {missing}")
                
                # Cr√©er l'objet
                target = EvaluationTarget(**target_dict)
                
                # Valider l'operator
                valid_ops = [">=", ">", "==", "<=", "<"]
                if target.operator not in valid_ops:
                    raise ValueError(f"Invalid operator: {target.operator}")
                
                targets.append(target)
            
            logger.info(f"‚úÖ Loaded {len(targets)} evaluation targets")
            return targets
            
        except FileNotFoundError as e:
            logger.error(f"‚ùå Config file not found: {e}")
            raise  # ‚ùå NE PAS IGNORER
        except yaml.YAMLError as e:
            logger.error(f"‚ùå Invalid YAML format: {e}")
            raise  # ‚ùå NE PAS IGNORER
        except Exception as e:
            logger.error(f"‚ùå Failed to load targets: {e}")
            raise  # ‚ùå NE PAS IGNORER

def _check_targets(self) -> bool:
    """V√©rifier que les targets d'√©valuation sont atteints."""
    try:
        targets = EvaluationTargetLoader.load_targets()
        
        results = []
        
        for target in targets:
            # R√©cup√©rer la valeur actuelle du benchmark
            current_value = self._get_benchmark_value(target.benchmark, target.metric)
            
            # √âvaluer contre le target
            passed = self._evaluate_operator(current_value, target.operator, target.value)
            
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            logger.info(f"{status}: {target.description}")
            logger.info(f"  {target.metric} = {current_value} {target.operator} {target.value}")
            
            results.append({
                'target': target,
                'current_value': current_value,
                'passed': passed
            })
        
        # R√©sum√©
        passed_count = sum(1 for r in results if r['passed'])
        total_count = len(results)
        
        logger.info(f"\nTarget Summary: {passed_count}/{total_count} passed")
        
        return passed_count == total_count
        
    except Exception as e:
        logger.error(f"‚ùå Failed to check targets: {e}")
        raise  # ‚ùå NE PAS IGNORER

def _get_benchmark_value(self, benchmark: str, metric: str) -> float:
    """R√©cup√©rer la valeur actuelle d'un benchmark."""
    if benchmark == "humaneval":
        return self.humaneval_results.get(metric, 0.0)
    elif benchmark == "mbpp":
        return self.mbpp_results.get(metric, 0.0)
    elif benchmark == "compliance":
        # Pour compliance, c'est 0-100%
        return self.compliance_results.get(metric, 0.0)
    else:
        raise ValueError(f"Unknown benchmark: {benchmark}")

def _evaluate_operator(self, current: float, operator: str, target: float) -> bool:
    """√âvaluer une condition avec l'operator sp√©cifi√©."""
    if operator == ">=":
        return current >= target
    elif operator == ">":
        return current > target
    elif operator == "==":
        return current == target
    elif operator == "<=":
        return current <= target
    elif operator == "<":
        return current < target
    else:
        raise ValueError(f"Unknown operator: {operator}")
```

---

## üéØ AM√âLIORER LOGIQUE PLANNING EVAL (1-2 heures)

### Fix #9: Planning Evaluation Robustness

**Fichier:** `eval/compliance_benchmark.py`

**Remplacer la logique simple par:**

```python
from dataclasses import dataclass
from typing import List, Dict, Set

@dataclass
class Task:
    """Repr√©sentation d'une t√¢che dans un plan."""
    id: str
    name: str
    dependencies: Set[str]  # IDs des t√¢ches dont elle d√©pend
    duration: float = 0.0

class PlanValidator:
    """Valider la structure et l'ex√©cution d'un plan."""
    
    @staticmethod
    def parse_plan_from_text(text: str) -> List[Task]:
        """Parser un plan depuis texte naturel du LLM."""
        # Impl√©menter un parser simple ou utiliser regex
        # Pour maintenant, c'est un placeholder
        # En production: utiliser un LLM pour extraire la structure
        pass
    
    @staticmethod
    def validate_dependencies(tasks: List[Task]) -> bool:
        """V√©rifier qu'il n'y a pas de cycle (DAG)."""
        visited = set()
        rec_stack = set()
        
        def has_cycle(task_id: str) -> bool:
            visited.add(task_id)
            rec_stack.add(task_id)
            
            task = next((t for t in tasks if t.id == task_id), None)
            if not task:
                return False
            
            for dep_id in task.dependencies:
                if dep_id not in visited:
                    if has_cycle(dep_id):
                        return True
                elif dep_id in rec_stack:
                    return True
            
            rec_stack.remove(task_id)
            return False
        
        # V√©rifier chaque t√¢che
        for task in tasks:
            if task.id not in visited:
                if has_cycle(task.id):
                    return False
        
        return True
    
    @staticmethod
    def validate_topological_order(tasks: List[Task]) -> bool:
        """V√©rifier l'ordre topologique du plan."""
        # V√©rifier que chaque t√¢che vient apr√®s ses d√©pendances
        task_positions = {t.id: i for i, t in enumerate(tasks)}
        
        for task in tasks:
            for dep_id in task.dependencies:
                if dep_id in task_positions:
                    if task_positions[dep_id] >= task_positions[task.id]:
                        # D√©pendance vient apr√®s la t√¢che!
                        return False
        
        return True
    
    @staticmethod
    def simulate_execution(tasks: List[Task]) -> Dict:
        """Simuler l'ex√©cution du plan."""
        completed = set()
        total_duration = 0.0
        
        # Ex√©cution topologique
        while len(completed) < len(tasks):
            found_ready = False
            
            for task in tasks:
                if task.id not in completed:
                    # V√©rifier si les d√©pendances sont compl√®tes
                    if task.dependencies.issubset(completed):
                        completed.add(task.id)
                        total_duration += task.duration
                        found_ready = True
            
            if not found_ready and len(completed) < len(tasks):
                # Deadlock!
                return {
                    'success': False,
                    'error': 'deadlock',
                    'completed': completed
                }
        
        return {
            'success': True,
            'completed': completed,
            'total_duration': total_duration
        }

def _evaluate_planning(self) -> bool:
    """
    √âvaluer la qualit√© du planning.
    
    Au lieu de chercher des keywords, on valide la structure r√©elle.
    """
    try:
        # 1. Parser le plan
        plan_text = self.agent.last_planning_output
        tasks = PlanValidator.parse_plan_from_text(plan_text)
        
        if not tasks or len(tasks) < 2:
            logger.error("Plan must contain at least 2 tasks")
            return False
        
        # 2. Valider qu'il n'y a pas de cycles
        if not PlanValidator.validate_dependencies(tasks):
            logger.error("Plan contains circular dependencies")
            return False
        
        # 3. Valider l'ordre topologique
        if not PlanValidator.validate_topological_order(tasks):
            logger.error("Tasks not in topological order")
            return False
        
        # 4. Simuler l'ex√©cution
        execution = PlanValidator.simulate_execution(tasks)
        
        if not execution['success']:
            logger.error(f"Plan execution failed: {execution.get('error')}")
            return False
        
        logger.info(f"‚úÖ Planning validation passed ({len(tasks)} tasks)")
        logger.info(f"   Total duration: {execution['total_duration']}s")
        
        return True
        
    except Exception as e:
        logger.error(f"Planning evaluation failed: {e}")
        return False
```

---

## ‚úÖ CHECKLIST FINALES

- [ ] Fix TaskNode import error (#164)
- [ ] Pin datasets dependency
- [ ] Fix ComplianceGuardian assertions
- [ ] Implement _evaluate_provenance()
- [ ] Implement _evaluate_audit_trail()
- [ ] Implement _evaluate_retention()
- [ ] Implement _evaluate_multi_step()
- [ ] Make eval targets data-driven
- [ ] Improve planning evaluation logic
- [ ] Enable branch coverage in pytest
- [ ] Run full test suite and verify
- [ ] Commit all changes to GitHub

---

## üöÄ R√âSUM√â

**Total Estimated Time:** 5-7 heures  
**Impact:** Production-ready compliance + robust evaluation system

D√®s que vous √™tes pr√™t √† impl√©menter, **dites-moi simplement "OK"** et je peux:
1. ‚úÖ G√©n√©rer les PR directement
2. ‚úÖ Vous expliquer chaque section
3. ‚úÖ Impl√©menter avec vous step-by-step

**√Ä demain avec de nouvelles priorit√©s!**
