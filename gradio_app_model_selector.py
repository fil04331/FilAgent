#!/usr/bin/env python3
"""
FilAgent - Interface Gradio avec S√©lection de Mod√®le Perplexity

Extension de l'interface production avec:
- S√©lection dynamique du mod√®le Perplexity
- Comparaison visuelle des r√©ponses
- M√©triques de performance par mod√®le
"""

import gradio as gr
import asyncio
import json
import uuid
import time
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Dict, Optional

# Ajouter le chemin du projet
sys.path.insert(0, str(Path(__file__).parent))

from runtime.model_interface import init_model, GenerationConfig, GenerationResult
from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# CONFIGURATION MOD√àLES PERPLEXITY
# ============================================================================

PERPLEXITY_MODELS = {
    "sonar": {
        "name": "Sonar (Rapide)",
        "full_name": "sonar",
        "description": "Mod√®le rapide pour questions simples",
        "use_case": "FAQ, calculs simples, recherche factuelle",
        "latency": "Tr√®s faible (<300ms)",
        "quality": "Bonne pour questions simples",
        "cost": "$",
        "features": ["Recherche web", "Rapide", "√âconomique"],
    },
    "sonar-pro": {
        "name": "Sonar Pro (√âquilibr√©)",
        "full_name": "sonar-pro",
        "description": "Mod√®le √©quilibr√© - RECOMMAND√â",
        "use_case": "Usage g√©n√©ral, analyses, conformit√©",
        "latency": "Faible (<500ms)",
        "quality": "Tr√®s bonne qualit√©/vitesse",
        "cost": "$$",
        "features": ["Recherche web avanc√©e", "Recommand√©"],
    },
    "sonar-reasoning": {
        "name": "Sonar Reasoning (Raisonnement)",
        "full_name": "sonar-reasoning",
        "description": "Raisonnement avec recherche web",
        "use_case": "Analyse complexe avec cha√Æne de pens√©e",
        "latency": "Mod√©r√©e (<1s)",
        "quality": "Excellence pour raisonnement",
        "cost": "$$$",
        "features": ["Recherche web", "Chain-of-thought", "Raisonnement"],
    },
    "sonar-reasoning-pro": {
        "name": "Sonar Reasoning Pro (Pr√©cis)",
        "full_name": "sonar-reasoning-pro",
        "description": "Raisonnement avanc√© (DeepSeek-R1)",
        "use_case": "D√©cisions critiques, analyse juridique approfondie",
        "latency": "√âlev√©e (<2s)",
        "quality": "Qualit√© maximale avec justification",
        "cost": "$$$$",
        "features": ["Recherche web", "DeepSeek-R1", "Qualit√© maximale"],
    },
    "sonar-deep-research": {
        "name": "Deep Research (Expert)",
        "full_name": "sonar-deep-research",
        "description": "Recherche approfondie niveau expert",
        "use_case": "Rapports d√©taill√©s, recherche exhaustive",
        "latency": "Tr√®s √©lev√©e (>10s)",
        "quality": "Rapports experts d√©taill√©s",
        "cost": "$$$$$",
        "features": ["Recherche exhaustive", "Rapports longs", "Expert"],
    },
}

# ============================================================================
# CLASSE GESTIONNAIRE DE MOD√àLES
# ============================================================================


class ModelManager:
    """Gestionnaire pour charger et utiliser diff√©rents mod√®les Perplexity"""

    def __init__(self):
        self.current_model = None
        self.current_model_name = None
        self.generation_config = GenerationConfig(temperature=0.7, max_tokens=2048, seed=42)

    def load_model(self, model_key: str) -> bool:
        """
        Charge un mod√®le Perplexity

        Args:
            model_key: Cl√© du mod√®le dans PERPLEXITY_MODELS

        Returns:
            True si chargement r√©ussi
        """
        try:
            model_info = PERPLEXITY_MODELS[model_key]
            full_name = model_info["full_name"]

            print(f"üîÑ Chargement du mod√®le: {full_name}")

            self.current_model = init_model(backend="perplexity", model_path=full_name, config={})

            self.current_model_name = model_key
            print(f"‚úÖ Mod√®le charg√©: {model_info['name']}")
            return True

        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le: {e}")
            return False

    def extract_sources(self, text: str) -> Tuple[str, List[str]]:
        """
        Extrait les sources cit√©es dans le texte Perplexity

        Returns:
            Tuple[str, List[str]]: (texte nettoy√©, liste des sources)
        """
        import re

        sources = []
        # Chercher les patterns de sources [1], [2], etc.
        citation_pattern = r"\[(\d+)\]"
        citations = re.findall(citation_pattern, text)

        # Pour l'instant, on garde les citations telles quelles
        # Les vraies URLs ne sont pas dans le texte mais dans les m√©tadonn√©es de l'API
        # On va formater pour rendre les citations cliquables visuellement

        return text, citations

    def clean_response(self, text: str, citation_urls: Optional[List[str]] = None) -> str:
        """
        Nettoie la r√©ponse en formatant les balises sp√©ciales pour Gradio

        Convertit les balises <think> en blocs format√©s et extrait le contenu final
        AFFICHE TOUT LE RAISONNEMENT (pas de troncature)
        Affiche les URLs r√©elles des sources si disponibles

        Args:
            text: Le texte de la r√©ponse
            citation_urls: Liste optionnelle des URLs de sources (depuis Perplexity API)
        """
        import re

        # Cas 1: Si la r√©ponse contient <think>...</think>, extraire et formater
        think_pattern = r"<think>(.*?)</think>\s*(.*)"
        match = re.search(think_pattern, text, flags=re.DOTALL)

        if match:
            think_content = match.group(1).strip()
            final_answer = match.group(2).strip()

            # Extraire les num√©ros de citations du texte
            clean_answer, citation_numbers = self.extract_sources(final_answer)

            # ‚úÖ AFFICHER TOUT LE RAISONNEMENT (enlever la limite de 500 caract√®res)
            # Formater le raisonnement en bloc d√©pliable
            formatted = f"""üí≠ **Cha√Æne de pens√©e compl√®te**:

<details>
<summary>üß† Cliquez pour voir le raisonnement d√©taill√© ({len(think_content)} caract√®res)</summary>

```
{think_content}
```
</details>

**üìù R√©ponse finale**:

{clean_answer}"""

            # Afficher les sources avec les URLs r√©elles si disponibles
            if citation_urls and citation_numbers:
                formatted += "\n\nüìö **Sources cit√©es**:\n\n"
                # Map citation numbers to URLs
                for num_str in sorted(set(citation_numbers)):
                    try:
                        idx = int(num_str) - 1  # [1] -> index 0
                        if 0 <= idx < len(citation_urls):
                            url = citation_urls[idx]
                            formatted += f"- [{num_str}] {url}\n"
                    except (ValueError, IndexError):
                        pass
            elif citation_numbers:
                # Fallback si pas d'URLs disponibles
                formatted += f"\n\nüìö **Sources cit√©es**: {', '.join([f'[{c}]' for c in sorted(set(citation_numbers))])}"

            return formatted

        # Cas 2: Pas de balises <think>, extraire quand m√™me les sources
        clean_text, citation_numbers = self.extract_sources(text)

        # Afficher les sources avec les URLs r√©elles si disponibles
        if citation_urls and citation_numbers:
            clean_text += "\n\nüìö **Sources cit√©es**:\n\n"
            for num_str in sorted(set(citation_numbers)):
                try:
                    idx = int(num_str) - 1  # [1] -> index 0
                    if 0 <= idx < len(citation_urls):
                        url = citation_urls[idx]
                        clean_text += f"- [{num_str}] {url}\n"
                except (ValueError, IndexError):
                    pass
        elif citation_numbers:
            # Fallback si pas d'URLs disponibles
            clean_text += f"\n\nüìö **Sources cit√©es**: {', '.join([f'[{c}]' for c in sorted(set(citation_numbers))])}"

        return clean_text

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> Tuple[str, Dict]:
        """
        G√©n√®re une r√©ponse avec le mod√®le actuel

        Returns:
            Tuple (r√©ponse, m√©triques)
        """
        if not self.current_model:
            return "‚ùå Aucun mod√®le charg√©", {}

        try:
            start_time = time.time()

            result: GenerationResult = self.current_model.generate(
                prompt=prompt, config=self.generation_config, system_prompt=system_prompt
            )

            generation_time = (time.time() - start_time) * 1000  # en ms

            metrics = {
                "model": PERPLEXITY_MODELS[self.current_model_name]["name"],
                "generation_time_ms": generation_time,
                "prompt_tokens": result.prompt_tokens,
                "completion_tokens": result.tokens_generated,
                "total_tokens": result.total_tokens,
                "finish_reason": result.finish_reason,
                "response_length": len(result.text),
            }

            # Nettoyer la r√©ponse pour affichage Gradio avec citations
            cleaned_text = self.clean_response(result.text, citation_urls=result.citations)

            return cleaned_text, metrics

        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration: {e}")
            return f"‚ùå Erreur: {str(e)}", {}


# ============================================================================
# INTERFACE GRADIO
# ============================================================================


class FilAgentModelSelector:
    """Interface Gradio avec s√©lection de mod√®le"""

    def __init__(self):
        self.model_manager = ModelManager()
        self.conversation_history = []

    def format_model_info(self, model_key: str) -> str:
        """Formate les informations d'un mod√®le pour affichage"""
        if model_key not in PERPLEXITY_MODELS:
            return "S√©lectionnez un mod√®le"

        info = PERPLEXITY_MODELS[model_key]

        features_str = " ‚Ä¢ ".join(info["features"])

        return f"""### {info['name']}

**Description**: {info['description']}

**Cas d'usage**: {info['use_case']}

**Caract√©ristiques**:
- Latence: {info['latency']}
- Qualit√©: {info['quality']}
- Co√ªt: {info['cost']}

**Fonctionnalit√©s**: {features_str}
"""

    def process_message(
        self, message: str, model_key: str, history: List[List[str]]
    ) -> Tuple[str, List[List[str]], str]:
        """
        Traite un message avec le mod√®le s√©lectionn√©

        Returns:
            Tuple (message_vide, historique_mis_√†_jour, m√©triques_format√©es)
        """
        if not message.strip():
            return "", history, ""

        if model_key not in PERPLEXITY_MODELS:
            error_msg = "‚ö†Ô∏è Veuillez s√©lectionner un mod√®le Perplexity"
            history.append([message, error_msg])
            return "", history, ""

        # Charger le mod√®le si n√©cessaire
        if self.model_manager.current_model_name != model_key:
            success = self.model_manager.load_model(model_key)
            if not success:
                error_msg = (
                    f"‚ùå Impossible de charger le mod√®le {PERPLEXITY_MODELS[model_key]['name']}"
                )
                history.append([message, error_msg])
                return "", history, ""

        # G√©n√©rer la r√©ponse
        response, metrics = self.model_manager.generate(prompt=message)

        # Mettre √† jour l'historique
        history.append([message, response])

        # Formater les m√©triques
        if metrics:
            metrics_text = f"""**M√©triques de g√©n√©ration**:
- Mod√®le: {metrics['model']}
- Temps: {metrics['generation_time_ms']:.0f} ms
- Tokens: {metrics['total_tokens']} ({metrics['prompt_tokens']} prompt + {metrics['completion_tokens']} completion)
- Longueur r√©ponse: {metrics['response_length']} caract√®res
- Statut: {metrics['finish_reason']}
"""
        else:
            metrics_text = ""

        return "", history, metrics_text

    def compare_models(self, query: str, model1_key: str, model2_key: str, model3_key: str) -> str:
        """
        Compare les r√©ponses de plusieurs mod√®les

        Returns:
            R√©sultats format√©s en Markdown
        """
        if not query.strip():
            return "‚ö†Ô∏è Veuillez entrer une question pour la comparaison"

        results_md = f"""# Comparaison de Mod√®les

**Question**: {query}

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

"""

        for model_key in [model1_key, model2_key, model3_key]:
            if not model_key or model_key not in PERPLEXITY_MODELS:
                continue

            model_info = PERPLEXITY_MODELS[model_key]
            results_md += f"""## {model_info['name']}

"""

            # Charger et tester le mod√®le
            success = self.model_manager.load_model(model_key)

            if not success:
                results_md += "‚ùå **Erreur de chargement du mod√®le**\n\n---\n\n"
                continue

            response, metrics = self.model_manager.generate(prompt=query)

            if metrics:
                results_md += f"""**M√©triques**:
- ‚è±Ô∏è Temps: {metrics['generation_time_ms']:.0f} ms
- üéØ Tokens: {metrics['total_tokens']}
- üìù Longueur: {metrics['response_length']} caract√®res

**R√©ponse**:
```
{response}
```

---

"""
            else:
                results_md += f"""**Erreur**: {response}

---

"""

        return results_md


def create_interface() -> gr.Blocks:
    """Cr√©e l'interface Gradio compl√®te"""

    app_instance = FilAgentModelSelector()

    with gr.Blocks(
        title="FilAgent - S√©lection de Mod√®le Perplexity",
        theme=gr.themes.Soft(primary_hue="blue", secondary_hue="gray"),
    ) as app:

        gr.Markdown("""
# ü§ñ FilAgent - S√©lecteur de Mod√®le Perplexity

### üéØ Testez et comparez les mod√®les Perplexity selon vos besoins
        """)

        with gr.Tabs():
            # ========== ONGLET CHAT ==========
            with gr.Tab("üí¨ Chat avec Mod√®le"):
                with gr.Row():
                    with gr.Column(scale=2):
                        model_selector = gr.Dropdown(
                            choices=list(PERPLEXITY_MODELS.keys()),
                            label="üéØ S√©lectionnez un mod√®le",
                            value="sonar-pro",
                            interactive=True,
                        )

                        model_info = gr.Markdown(app_instance.format_model_info("sonar-pro"))

                        chatbot = gr.Chatbot(
                            label="Conversation", height=400, show_copy_button=True
                        )

                        with gr.Row():
                            msg_input = gr.Textbox(
                                label="Message",
                                placeholder="Posez votre question...",
                                lines=2,
                                scale=4,
                            )
                            send_btn = gr.Button("üì§ Envoyer", variant="primary", scale=1)

                        clear_btn = gr.Button("üóëÔ∏è Effacer la conversation")

                        gr.Examples(
                            examples=[
                                "Quelle est la capitale du Qu√©bec?",
                                "Explique la Loi 25 du Qu√©bec en 3 points",
                                "Calcule TPS (5%) et TVQ (9.975%) sur 1500$",
                                "Quels sont les risques d'utiliser l'IA sans conformit√©?",
                            ],
                            inputs=msg_input,
                            label="üí° Questions exemples",
                        )

                    with gr.Column(scale=1):
                        gr.Markdown("### üìä M√©triques")
                        metrics_display = gr.Markdown("")

                        gr.Markdown("### ‚ÑπÔ∏è Guide de s√©lection")
                        gr.Markdown("""
**Faible difficult√©**:
- sonar-small (rapide)
- 8b-instruct (√©conomique)

**Moyen difficult√©**:
- sonar-large (√©quilibr√©)

**Haute difficult√©**:
- sonar-huge (pr√©cis)
- 70b-instruct (puissant)
                        """)

                # Connexions √©v√©nements
                model_selector.change(
                    fn=app_instance.format_model_info, inputs=[model_selector], outputs=[model_info]
                )

                msg_input.submit(
                    fn=app_instance.process_message,
                    inputs=[msg_input, model_selector, chatbot],
                    outputs=[msg_input, chatbot, metrics_display],
                )

                send_btn.click(
                    fn=app_instance.process_message,
                    inputs=[msg_input, model_selector, chatbot],
                    outputs=[msg_input, chatbot, metrics_display],
                )

                clear_btn.click(fn=lambda: ([], ""), outputs=[chatbot, metrics_display])

            # ========== ONGLET COMPARAISON ==========
            with gr.Tab("‚öñÔ∏è Comparaison de Mod√®les"):
                gr.Markdown("""
## Comparez les r√©ponses de plusieurs mod√®les

Testez la m√™me question avec diff√©rents mod√®les pour voir les diff√©rences de qualit√©,
vitesse et style de r√©ponse.
                """)

                compare_query = gr.Textbox(
                    label="Question √† tester", placeholder="Entrez votre question...", lines=3
                )

                with gr.Row():
                    compare_model1 = gr.Dropdown(
                        choices=list(PERPLEXITY_MODELS.keys()), label="Mod√®le 1", value="sonar"
                    )
                    compare_model2 = gr.Dropdown(
                        choices=list(PERPLEXITY_MODELS.keys()), label="Mod√®le 2", value="sonar-pro"
                    )
                    compare_model3 = gr.Dropdown(
                        choices=list(PERPLEXITY_MODELS.keys()),
                        label="Mod√®le 3",
                        value="sonar-reasoning",
                    )

                compare_btn = gr.Button("üîç Comparer les mod√®les", variant="primary", size="lg")

                comparison_results = gr.Markdown(label="R√©sultats de comparaison")

                gr.Examples(
                    examples=[
                        "Explique la diff√©rence entre la Loi 25 et le RGPD",
                        "Une PME veut utiliser l'IA pour analyser des CV. Quelles sont les obligations l√©gales au Qu√©bec?",
                        "Compare les avantages et inconv√©nients de 3 tailles de mod√®les LLM (petit, moyen, grand)",
                    ],
                    inputs=compare_query,
                    label="üí° Questions de comparaison",
                )

                compare_btn.click(
                    fn=app_instance.compare_models,
                    inputs=[compare_query, compare_model1, compare_model2, compare_model3],
                    outputs=[comparison_results],
                )

            # ========== ONGLET INFO MOD√àLES ==========
            with gr.Tab("üìö Informations Mod√®les"):
                gr.Markdown("""
## Tous les Mod√®les Perplexity Disponibles

Chaque mod√®le a ses forces et cas d'usage sp√©cifiques.
                """)

                for model_key, model_info in PERPLEXITY_MODELS.items():
                    with gr.Accordion(
                        f"{model_info['name']} - {model_info['description']}", open=False
                    ):
                        gr.Markdown(f"""
**Nom complet**: `{model_info['full_name']}`

**Cas d'usage**: {model_info['use_case']}

**Caract√©ristiques**:
- **Latence**: {model_info['latency']}
- **Qualit√©**: {model_info['quality']}
- **Co√ªt relatif**: {model_info['cost']}

**Fonctionnalit√©s**: {', '.join(model_info['features'])}

---

**Quand l'utiliser?**

{model_info['description']}
                        """)

    return app


# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    import os

    # V√©rifier la cl√© API
    if not os.getenv("PERPLEXITY_API_KEY"):
        print("‚ùå ERREUR: PERPLEXITY_API_KEY non d√©finie dans .env")
        sys.exit(1)

    print("=" * 60)
    print("üöÄ Lancement de FilAgent Model Selector")
    print("=" * 60)

    app = create_interface()

    app.launch(server_name="0.0.0.0", server_port=7861, share=False, show_error=True)
