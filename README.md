# **LLM-Agent : Gouvernance & Tra√ßabilit√©**

Ce projet vise √† d√©velopper un agent bas√© sur un Grand Mod√®le de Langage (LLM) avec un accent fondamental sur la gouvernance, la tra√ßabilit√© l√©gale, la s√©curit√© et la reproductibilit√© des d√©cisions. L'architecture est con√ßue pour √™tre ex√©cutable localement tout en respectant des normes de conformit√© strictes (Loi 25 du Qu√©bec, AI Act de l'UE, NIST AI RMF).

## **üéØ Objectifs Principaux**

1. **M√©moire G√©r√©e :** Mettre en place une m√©moire √† court et long terme avec un contr√¥le strict des consentements et une minimisation des donn√©es.  
2. **Interpr√©tations Logu√©es :** Assurer que chaque action, d√©cision et interaction de l'agent soit enregistr√©e dans un journal immuable (WORM) et analysable.  
3. **Tra√ßabilit√© L√©gale :** Garantir la capacit√© de reconstruire et d'expliquer une d√©cision automatis√©e, conform√©ment aux exigences l√©gales (notamment l'ADM de la Loi 25).  
4. **Capacit√©s d'Agent Avanc√©es :** Viser des performances √©gales ou sup√©rieures aux standards de l'industrie (Codex, agents avanc√©s) sur des t√¢ches de raisonnement et de manipulation d'outils, valid√©es par des benchmarks rigoureux.

## **‚ú® Fonctionnalit√©s Cl√©s**

* **Moteur de Politiques (Policy Engine) :** Contr√¥le en amont de chaque action pour le masquage de PII, la v√©rification des droits et la conformit√© juridictionnelle.  
* **Ex√©cution S√©curis√©e (Sandboxing) :** Isolation compl√®te de l'ex√©cution de code et des outils pour pr√©venir les risques de s√©curit√©.  
* **Journalisation WORM :** Utilisation de cha√Ænes de hachage (Merkle Tree) pour garantir l'int√©grit√© et l'immuabilit√© des journaux d'audit.  
* **G√©n√©ration de "Dossiers de D√©cision" :** Capacit√© √† produire un rapport structur√© pour chaque d√©cision, expliquant les entr√©es, les r√®gles appliqu√©es et les sorties.  
* **Reproductibilit√© :** Versionnement strict des mod√®les, des param√®tres (seed, temp√©rature) et des politiques pour garantir des r√©sultats reproductibles.  
* **√âvaluation Continue :** Int√©gration d'un harnais d'√©valuation continue bas√© sur des benchmarks standards (HumanEval, MBPP, SWE-bench-lite).

## **üõ†Ô∏è Architecture Technique**

Le projet est structur√© autour d'une arborescence claire, s√©parant la configuration, les mod√®les, la m√©moire, les logs, les outils et l'√©valuation.

* **Inf√©rence Flexible :** Support de multiples backends LLM :
  - **Local** : llama.cpp ou vLLM pour garantir la confidentialit√© des donn√©es
  - **Cloud** : Perplexity API avec recherche web en temps r√©el
* **M√©moire Hybride :** Combine une m√©moire √©pisodique (SQLite) pour le contexte de la conversation et une m√©moire s√©mantique (FAISS/Parquet) pour la recherche de connaissances √† long terme.
* **Observabilit√© :** Les logs sont structur√©s au format JSONL compatible OpenTelemetry pour une analyse et une surveillance facilit√©es.
* **Provenance :** Chaque artefact g√©n√©r√© est accompagn√© de m√©tadonn√©es de provenance suivant le standard W3C PROV-JSON.

### **Choix du Backend LLM**

FilAgent supporte deux modes d'ex√©cution selon vos besoins :

#### **Option 1 : Perplexity API (Cloud)** - Configuration actuelle

**Avantages :**
- D√©marrage rapide sans t√©l√©chargement de mod√®le
- Recherche web en temps r√©el int√©gr√©e (mod√®les Sonar)
- Performance √©lev√©e sur t√¢ches complexes
- Pas de mat√©riel GPU requis

**Pr√©requis :**
- Cl√© API Perplexity (obtenir sur https://www.perplexity.ai/settings/api)
- Connexion Internet stable

**Statut :** Backend actuellement configur√© et fonctionnel avec Perplexity API.

#### **Option 2 : Mod√®le Local (llama.cpp)** - Priv√©

**Avantages :**
- Confidentialit√© maximale (100% local)
- Pas de d√©pendance Internet
- Conformit√© stricte pour donn√©es sensibles
- Co√ªts d'op√©ration r√©duits

**Pr√©requis :**
- T√©l√©chargement mod√®le GGUF (~4-8 GB selon quantisation)
- 8+ GB RAM (16GB recommand√©)
- GPU optionnel mais recommand√© pour performance

**Note :** Les deux backends garantissent la m√™me conformit√© (Loi 25, GDPR, AI Act) gr√¢ce aux middlewares de gouvernance int√©gr√©s.

## **üöÄ D√©marrage Rapide (Getting Started)**

### Pr√©requis

- Python 3.10 ou sup√©rieur
- Git
- 8+ GB de RAM (16GB recommand√©)
- Optionnel : GPU NVIDIA pour acc√©l√©ration

### Installation

```bash
# 1. Cloner le d√©p√¥t
git clone https://github.com/votre-org/FilAgent.git
cd FilAgent

# 2. Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. Choisir votre backend LLM

# Option A: Perplexity API (RECOMMAND√â pour d√©marrage rapide)
# - Copier le fichier .env.example vers .env
cp .env.example .env
# - √âditer .env et ajouter votre cl√© API Perplexity
#   LLM_BACKEND=perplexity
#   PERPLEXITY_API_KEY=pplx-votre-cle-ici
# - Aucun t√©l√©chargement de mod√®le requis
# Voir docs/PERPLEXITY_INTEGRATION.md pour configuration d√©taill√©e

# Option B: Mod√®le local llama.cpp (pour confidentialit√© maximale)
# - T√©l√©charger un mod√®le GGUF
mkdir -p models/weights
cd models/weights
wget https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF/resolve/main/llama-3-8b-instruct.Q4_K_M.gguf -O base.gguf
cd ../..
# - Configurer .env avec LLM_BACKEND=llama.cpp
# Voir models/weights/README.md pour plus de mod√®les disponibles

# 5. Initialiser la base de donn√©es
python -c "from memory.episodic import create_tables; create_tables()"
```

### Configuration

Les configurations par d√©faut sont dans `config/`. Vous pouvez les ajuster :

- `config/agent.yaml` : Param√®tres de g√©n√©ration, mod√®le, m√©moire
- `config/policies.yaml` : R√®gles d'usage, RBAC, guardrails
- `config/retention.yaml` : Politiques de r√©tention des donn√©es
- `config/provenance.yaml` : Configuration de tra√ßabilit√©
- `config/eval_targets.yaml` : Seuils d'√©valuation

### Lancement

```bash
# Lancer le serveur API
python runtime/server.py

# Le serveur sera accessible sur http://localhost:8000
# Documentation API sur http://localhost:8000/docs
```

### Test rapide

```bash
# Test avec Perplexity (backend actuel)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Bonjour ! Peux-tu me confirmer que tu fonctionnes ?"}],
    "conversation_id": "test-123"
  }'

# Test avec mod√®le local (si configur√©)
# M√™me commande - le backend est transparent pour l'utilisateur
```

**Guides additionnels :**
- [QUICK_TEST.md](QUICK_TEST.md) - Guide complet de tests post-installation
- [README_DEPLOYMENT.md](README_DEPLOYMENT.md) - Guide de d√©ploiement en production
- [docs/PERPLEXITY_INTEGRATION.md](docs/PERPLEXITY_INTEGRATION.md) - Configuration d√©taill√©e Perplexity

## **‚öñÔ∏è Conformit√© et Gouvernance**

Ce projet int√®gre d√®s sa conception les cadres de gouvernance suivants :

* **ISO/IEC 42001 :** Syst√®me de management de l'IA.  
* **NIST AI RMF 1.0 :** Cadre de gestion des risques de l'IA.  
* **Loi 25 (Qu√©bec) :** Transparence et explicabilit√© des d√©cisions automatis√©es.  
* **AI Act (UE) :** Exigences de tra√ßabilit√© et de transparence.